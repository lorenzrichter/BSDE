{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation of example V.B. of the paper *Variational approach to rare event simulation using least-squares regression, C. Hartmann, O. Kebiri, L. Neureither, L. Richter,\n",
    "submitted to Chaos, 2019.*\n",
    "\n",
    "We consider the Ornstein-Uhlenbeck SDE\n",
    "$$d X_t = (\\mu-X_t ) dt + \\sigma d B_t$$\n",
    "and want to compute the expectation value\n",
    "$$\\mathbb{E}[\\exp(-\\alpha X_T) | X_0 = x]$$\n",
    "by approximating the corresponding optimal control in an importance sampling attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import time\n",
    "import torch as pt \n",
    "\n",
    "from numpy import exp, sqrt\n",
    "from scipy.linalg import inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shooting method with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        self.nn_dims = [1, 20, 1]\n",
    "        self.W = [item for sublist in \n",
    "                  [[pt.nn.Parameter(pt.randn(self.nn_dims[i], self.nn_dims[i + 1], requires_grad=True)),\n",
    "                    pt.nn.Parameter(pt.randn(self.nn_dims[i + 1], requires_grad=True))] for\n",
    "                   i in range(len(self.nn_dims) - 1)]\n",
    "                  for item in sublist]\n",
    "        for i, w in enumerate(self.W):\n",
    "            self.register_parameter('param %d' % i, w)\n",
    "        \n",
    "        self.BN1 = pt.nn.BatchNorm1d(self.nn_dims[0])\n",
    "        self.BN2 = pt.nn.BatchNorm1d(self.nn_dims[1])\n",
    "        self.BN3 = pt.nn.BatchNorm1d(self.nn_dims[2])\n",
    "        self.BN = [self.BN1, self.BN2, self.BN3]\n",
    "        \n",
    "        self.adam = pt.optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.BN[0](x)\n",
    "        for i in range(len(self.nn_dims) - 1):\n",
    "            x = pt.matmul(x, self.W[2 * i]) # + self.W[2 * i + 1]\n",
    "            x = self.BN[i + 1](x)\n",
    "            if i != len(self.nn_dims) - 2:\n",
    "                x = pt.nn.functional.relu(x)\n",
    "        return x\n",
    "    \n",
    "class Y_0(pt.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Y_0, self).__init__()\n",
    "        self.Y_0 = pt.nn.Parameter(pt.randn(1), requires_grad=True)\n",
    "        self.register_parameter('param', self.Y_0)\n",
    "        self.adam = pt.optim.Adam(self.parameters(), lr=0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return pt.ones(x.shape[0]) * self.Y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = 1\n",
    "\n",
    "def u_t(u, t, T):\n",
    "    return -sigma.item() * u * exp(t - T)\n",
    "\n",
    "def gamma_t(u, t, T, x):\n",
    "    return u * x * exp(t - T) - u**2 * sigma**2 / 4 * (1 - exp(2 * (t - T)))\n",
    "\n",
    "def g(x):\n",
    "    return u * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5\n",
    "sigma = pt.sqrt(pt.tensor(2.0))\n",
    "\n",
    "delta_t = pt.tensor(0.05)    # time discretization of SDE\n",
    "sq_delta_t = pt.sqrt(delta_t)\n",
    "N = int(T / delta_t)\n",
    "\n",
    "K = 50    # batch size\n",
    "L = 10000    # gradient steps\n",
    "\n",
    "nn_y = Y_0()\n",
    "nn_z = [NN() for i in range(N)]\n",
    "\n",
    "sd_nn_y = copy.deepcopy(nn_y.state_dict())\n",
    "sd_nn_z = [copy.deepcopy(nn.state_dict()) for nn in nn_z]\n",
    "\n",
    "nn_y.load_state_dict(sd_nn_y)\n",
    "for nn, sd_nn in zip(nn_z, sd_nn_z):\n",
    "    nn.load_state_dict(sd_nn)\n",
    "\n",
    "nn_y.train()\n",
    "for nn in nn_z:\n",
    "    nn.train()\n",
    "\n",
    "loss_log = []\n",
    "Y_0_log = []\n",
    "times = []\n",
    "\n",
    "for l in range(L):\n",
    "    t_0 = time.time()\n",
    "    X = pt.zeros([K, N + 1])\n",
    "    Y = pt.zeros([K, N + 1])\n",
    "    X[:, 0] = pt.zeros(K)\n",
    "    Y[:, 0] = nn_y.forward(X[:, 0])\n",
    "\n",
    "    xi = pt.randn(K, N + 1)\n",
    "    for i in range(N):\n",
    "        Z = -nn_z[i].forward(X[:, i].unsqueeze(1)).squeeze(1)\n",
    "        # c = 0\n",
    "        c = -Z\n",
    "        # c = u_t(u, i * delta_t, T)\n",
    "        X[:, i+1] = X[:, i] + (- X[:, i] + sigma * c) * delta_t + sigma * sq_delta_t * xi[:, i+1] # - sigma * Z\n",
    "        Y[:, i+1] = Y[:, i] + delta_t * (0.5 * Z.pow(2) + c * Z) + sq_delta_t * Z * xi[:, i+1] # - 0.5\n",
    "    \n",
    "    nn_y.adam.zero_grad()\n",
    "    for nn in nn_z:\n",
    "        nn.adam.zero_grad()\n",
    "\n",
    "    loss = (Y[:, N] - g(X[:, N])).pow(2).mean()\n",
    "    loss.backward()\n",
    "\n",
    "    nn_y.adam.step()\n",
    "    for nn in nn_z:\n",
    "        nn.adam.step()   \n",
    "\n",
    "    loss_log.append(pt.mean((Y[:, N] - g(X[:, N]))**2).item())\n",
    "    \n",
    "    t_1 = time.time()\n",
    "    times.append(t_1 - t_0)\n",
    "    Y_0_log.append(Y[0, 0].item())\n",
    "    \n",
    "    if (l % 100 == 0):\n",
    "        print('%d - loss: %.4e - Y_0: %.4e - time per iteration: %.2fs' % (l, loss_log[-1], Y[0, 0].item(), np.mean(times[-100:])))\n",
    "        \n",
    "#loss_log_NN_naive = loss_log.copy()\n",
    "loss_log_NN_control = loss_log.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ansatz functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_gaussian = 5\n",
    "mus = np.linspace(-2, 2, M_gaussian)\n",
    "sigma_basis = 1\n",
    "\n",
    "def phi_m_gaussian(x, m):\n",
    "    return 1 / sqrt(2 * np.pi * sigma_basis**2) * exp(- (x - mus[m])**2 / (2 * sigma_basis**2))\n",
    "\n",
    "def phi_constant(x):\n",
    "    return np.ones(len(x))\n",
    "\n",
    "def phi(x, ansatz):\n",
    "    if ansatz == 'gaussian':\n",
    "        return [phi_m_gaussian(x, m) for m in range(M_gaussian)]\n",
    "    if ansatz == 'constant':\n",
    "        return [phi_constant(x)]\n",
    "\n",
    "def grad_phi_m(x, m):\n",
    "    return -(x - mus[m]) / (sqrt(2 * np.pi) * sigma_basis**3) * exp(- (x - mus[m])**2 / (2 * sigma_basis**2))\n",
    "\n",
    "def grad_phi_gaussian(x):\n",
    "    return [grad_phi_m(x, m) for m in range(M_gaussian)]\n",
    "\n",
    "def gamma(x, n):\n",
    "    return alpha[n, :].dot(phi(x))\n",
    "\n",
    "def grad_gamma(x, n, alpha, ansatz):\n",
    "    return alpha[n, :].dot(phi(x, ansatz))\n",
    "\n",
    "def grad_gamma_LSMC_gaussian(x, n, alpha):\n",
    "    return alpha[n, :].dot(grad_phi_gaussian(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = np.linspace(-4, 4, 400);\n",
    "\n",
    "for m in range(M_gaussian):\n",
    "    plt.plot(x_val, phi_m_gaussian(x_val, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shooting method with ansatz functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansatz = 'gaussian'\n",
    "M = 5    # amount of ansatz functions\n",
    "T = 5    # time horizon\n",
    "eta = 0.05    # learning rate\n",
    "sigma = sqrt(2.0)    # noise of SDE\n",
    "\n",
    "delta_t = 0.05    # time discretization of SDE\n",
    "N = int((T / delta_t) + 1)\n",
    "K = 100    # batch size\n",
    "L = 1000000    # gradient steps\n",
    "\n",
    "X = np.zeros([K, N + 1], dtype=np.float32)\n",
    "Y = np.zeros([K, N + 1], dtype=np.float32)\n",
    "Z = np.zeros([K, N + 1], dtype=np.float32)\n",
    "alpha = np.zeros([N + 1, M], dtype=np.float32)\n",
    "alpha_Y = 0\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for l in range(L):\n",
    "    X[:, 0] = np.zeros(K)    # np.random.uniform(K)\n",
    "    Y[:, 0] = np.ones(K) * alpha_Y\n",
    "    xi = np.random.randn(K, N + 1)\n",
    "    for i in range(N):\n",
    "        X[:, i+1] = X[:, i] + (- X[:, i]) * delta_t + sigma * sqrt(delta_t) * xi[:, i+1]\n",
    "        Y[:, i+1] = (Y[:, i] + delta_t * 0.5 * (sigma * grad_gamma(X[:, i], i, alpha, ansatz))**2\n",
    "                     + sqrt(delta_t) * sigma * grad_gamma(X[:, i], i, alpha, ansatz) * xi[:, i+1])\n",
    "    \n",
    "    # compute gradients and update\n",
    "    for i in range(N):\n",
    "        for m in range(M):\n",
    "            alpha[i, m] -= eta * np.mean(2 * (Y[:, N] - g(X[:, N])) * (delta_t * sigma**2 * grad_gamma(X[:, i], i, alpha, ansatz) * phi(X[:, i], ansatz)[m] + sqrt(delta_t) * sigma * phi(X[:, i], ansatz)[m] * xi[:, i+1]))\n",
    "    \n",
    "    alpha_Y -= eta * np.mean(2 * (Y[:, N] - g(X[:, N])))\n",
    "    \n",
    "    loss_log.append(np.mean((Y[:, N] - g(X[:, N]))**2))\n",
    "        \n",
    "    if (l % 100 == 0):\n",
    "        print(\"%d - loss: %.4e\" % (l, loss_log[l]))\n",
    "        \n",
    "#alpha_constant = alpha.copy()\n",
    "#loss_log_ansatz = loss_log.copy()\n",
    "alpha_gaussian = alpha.copy()\n",
    "loss_log_gaussian = loss_log.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSMC with ansatz functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_m_linear(x, m):\n",
    "    if m == 0:\n",
    "        return np.ones(len(x))\n",
    "    if m == 1:\n",
    "        return x\n",
    "    \n",
    "def grad_phi_m_linear(x, m):\n",
    "    if m == 0:\n",
    "        return np.zeros(len(x))\n",
    "    if m == 1:\n",
    "        return np.ones(len(x))\n",
    "    \n",
    "def grad_phi(x):\n",
    "    return [grad_phi_m_linear(x, m) for m in range(2)]\n",
    "\n",
    "def grad_gamma_LSMC_linear(x, n, alpha):\n",
    "    return alpha[n, :].dot(grad_phi(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = M_gaussian    # amount of ansatz functions\n",
    "#M = 2\n",
    "T = 5    # time horizon\n",
    "K = 10000    # samples\n",
    "delta_t = 0.05    # time discretization of SDE\n",
    "N = int(T / delta_t)\n",
    "\n",
    "X = np.zeros([K, N + 1])\n",
    "Y = np.zeros([K, N + 1])\n",
    "Z = np.zeros([K, N + 1])\n",
    "alpha = np.zeros([N + 1, M])\n",
    "\n",
    "X[:, 0] = 0.0 * np.ones(K)\n",
    "\n",
    "for i in range(N):\n",
    "    xi = np.random.randn(K)\n",
    "    X[:, i+1] = X[:, i] + (- X[:, i]) * delta_t + sigma * sqrt(delta_t) * xi\n",
    "\n",
    "Y[:, N] = g(X[:, N])\n",
    "Z[:, N] = sigma * u * np.ones(K)\n",
    "\n",
    "\n",
    "for n in range(N-1, 0, -1):\n",
    "\n",
    "    A_n = np.zeros([K, M])\n",
    "    for m in range(M):\n",
    "        A_n[:, m] = phi_m_gaussian(X[:, n], m)\n",
    "        # A_n[:, m] = phi_m_linear(X[:, n], m)\n",
    "        \n",
    "    b_n = Y[:, n+1] - delta_t * 0.5 * Z[:, n+1]**2\n",
    "    alpha[n, :] = inv(A_n.T.dot(A_n)).dot(A_n.T).dot(b_n)\n",
    "    Y[:, n] = A_n.dot(alpha[n, :])\n",
    "    \n",
    "    A_n = np.zeros([K, M])\n",
    "    for m in range(M):\n",
    "        A_n[:, m] = grad_phi_m(X[:, n], m)\n",
    "        # A_n[:, m] = grad_phi_m_linear(X[:, n], m)\n",
    "    \n",
    "    Z[:, n] = sigma * A_n.dot(alpha[n, :])\n",
    "    \n",
    "alpha_LSMC_gaussian = copy.deepcopy(alpha)\n",
    "#alpha_LSMC_linear = copy.deepcopy(alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_y.eval()\n",
    "for nn in nn_z:\n",
    "    nn.eval()\n",
    "    \n",
    "x = -1.0    # plotted x value\n",
    "t = 3    # plotted t value\n",
    "n = int(np.ceil(t / delta_t))\n",
    "\n",
    "plt.rcParams.update({'font.size': 13})\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(6, 8)) \n",
    "t_range = np.linspace(0, T, N + 1)\n",
    "n_range = range(0, N + 1)\n",
    "x_range = np.linspace(-3, 3, 100)\n",
    "\n",
    "ax[0].set_title('optimal control for x = %.0f' % x)\n",
    "ax[0].plot(t_range[1:-1], - sigma * grad_gamma_LSMC_linear([x], n_range[1:-1], alpha_LSMC_linear), '.', ms=8, markevery=4, label='LSMC, constant ansatz', color='blue')\n",
    "ax[0].plot(t_range[1:-1], - sigma * grad_gamma_LSMC_gaussian([x], n_range[1:-1], alpha_LSMC_gaussian), '--', linewidth=1, label='LSMC, Gaussian ansatz', color='blue')\n",
    "ax[0].plot(t_range[2:], [nn_z[n].forward((pt.tensor([[x]]))).item() for n in n_range[1:-1]], 'x', ms=10, mew=2, markevery=4, label='shooting, NN', color='green');\n",
    "ax[0].plot(t_range, - sigma * grad_gamma([x], np.array(n_range), alpha_constant[:-1], 'constant'), '.', ms=8, markevery=4, label='shooting, constant ansatz', color='red')\n",
    "ax[0].plot(t_range, - sigma * grad_gamma([x], np.array(n_range), alpha_gaussian[:-1], 'gaussian'), '--', linewidth=1, ms=7, markevery=4, label='shooting, Gaussian ansatz', color='red')\n",
    "ax[0].plot(t_range, u_t(u, t_range, T), color='grey', label=r'true solution $u^*(x, t)$');\n",
    "ax[0].set_xlabel('t');\n",
    "ax[0].set_ylabel(r'$u^*(x, t)$');\n",
    "ax[0].legend();\n",
    "\n",
    "ax[1].set_title('optimal control for t = %.0f' % t)\n",
    "ax[1].plot(x_range, - sigma * grad_gamma_LSMC_linear(x_range, n + 1, alpha_LSMC_linear), '.', ms=8, markevery=5, label='LSMC, constant ansatz', color='blue')\n",
    "ax[1].plot(x_range, - sigma * grad_gamma_LSMC_gaussian(x_range, n + 1, alpha_LSMC_gaussian), '--', linewidth=1, label='LSMC, Gaussian ansatz', color='blue')\n",
    "ax[1].plot(x_range, [nn_z[n].forward((pt.tensor([[x]]))).item() for x in x_range], 'x', ms=10, mew=2, markevery=4, label='shooting, NN', color='green');\n",
    "ax[1].plot(x_range, - sigma * grad_gamma(x_range, n + 1, alpha_constant, 'constant'), '.', ms=8, markevery=6, label='sgooting, constant ansatz', color='red');\n",
    "ax[1].plot(x_range, - sigma * grad_gamma(x_range, n + 1, alpha_gaussian, 'gaussian'), '--', linewidth=1, ms=7, markevery=4, label='shooting, Gaussian ansatz', color='red');\n",
    "ax[1].plot(x_range, np.ones(len(x_range)) * u_t(u, t, T), color='grey', label=r'true solution $u^*(x, t)$');\n",
    "#ax[1].legend()\n",
    "ax[1].set_xlabel('x');\n",
    "ax[1].set_ylabel(r'$u^*(x, t)$');\n",
    "ax[1].set_ylim(-0.3, 0);\n",
    "fig.tight_layout()\n",
    "#fig.savefig('img/OU_approximation_comparison.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "start = 0\n",
    "end = 1700\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "ax.plot(loss_log_naive[start:end], label='NN, uncontrolled')\n",
    "ax.plot(loss_log_NN_control[start:end], label='NN, controlled', color='grey')\n",
    "ax.plot(loss_log_ansatz[start:end], label='constant ansatz', color='orange')\n",
    "ax.plot(loss_log_gaussian[start:end], label='Gaussian ansatz', color='green')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('gradient steps');\n",
    "ax.set_ylabel('loss');\n",
    "ax.legend();\n",
    "#fig.savefig('img/OU_loss_comparison.pdf');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
